<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://etienne.boisseau.eu/feed.xml" rel="self" type="application/atom+xml"/><link href="https://etienne.boisseau.eu/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-15T10:57:59+00:00</updated><id>https://etienne.boisseau.eu/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">VWT: The Inspection Paradox as a Probability Transform</title><link href="https://etienne.boisseau.eu/blog/2026/vwt/" rel="alternate" type="text/html" title="VWT: The Inspection Paradox as a Probability Transform"/><published>2026-02-08T00:00:00+00:00</published><updated>2026-02-08T00:00:00+00:00</updated><id>https://etienne.boisseau.eu/blog/2026/vwt</id><content type="html" xml:base="https://etienne.boisseau.eu/blog/2026/vwt/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>The inspection paradox (also called length-biased sampling) is a statistical paradox whereby the same probability distribution is altered depending on who measures it. Specifically, it occurs when the observer of a probability distribution is themselves an individual which the distribution tries to count, which biases their sampling of it towards larger values.</p> <p>For a concrete example, consider waiting for a bus which comes, on average, once every 10 minutes. You might expect that your average waiting time will be 10/2 = 5 minutes. In fact, this will only be true if the buses arrive <em>exactly</em> once every 10 minutes.</p> <p>Consider what happens if the interval between two bus arrivals has a 50-50 chance of being either exactly 5 minutes or exactly 15 minutes. The bus still comes on average once every 10 minutes. But since you arrive at the bus stop at a random time with respect to the bus schedule, your likelihood to arrive within the 15-minute interval is 3 times higher than your likelihood to arrive within the 5-minute interval, meaning the probabilities from your perspective are actually 25-75.</p> <p>Your actual average waiting time is therefore:</p> \[\text{25%}\times \frac{5}{2} + \text{75%}\times \frac{15}{2} = 6.25\text{ minutes}\] <p>To convince yourself of this discrepancy, imagine an extreme case: when two buses arrive at the exact same moment. The average time between bus arrivals goes down since we just observed a 0-second waiting time interval. But no passenger’s actual waiting time was reduced in the process.</p> <h2 id="formal-derivation">Formal derivation</h2> <p>To formalize this phenomenon, we introduce the Value-Weighted Transform (VWT), a probability transform which maps a distribution supported on $\R^{+}$ from its original form to what it looks like as observed from one of the individuals counted by it.</p> <p>If $f_X$ is the density of a distribution with finite expectation supported on $\R^{+}$ with respect to some measure $\lambda$, then $f_Y$, the density of the Value-Weighted Transform of this distribution, can be written as:</p> \[\forall x\in\R^{+}, f_Y(x) = \frac{x}{\E(X)}f_X(x)\] <p>It is easy to prove that:</p> \[\begin{align*} \forall n \in \N, &amp;&amp; \E(Y^n) &amp;= \int_{\R} x^n \frac{x}{\E(X)} \,df_X(x) \\ &amp;&amp;&amp;= \frac{\E(X^{n+1})}{\E(X)} \end{align*}\] <p>And in particular:</p> \[\begin{align*} \E(Y) &amp;= \frac{\E(X^2)}{\E(X)} \\ &amp; = \E(X) + \frac{\text{Var}(X)}{\E(X)} \end{align*}\] <p>This means that the expectation of $Y$ is always greater than or equal to the expectation of $X$, with equality only when $X$ is almost-surely a constant (i.e. when there is no variability in the distribution). This formalizes the intuition behind the inspection paradox: when there is variability in the distribution, the individuals counted by it are more likely to observe larger values.</p> <p>We can also express the characteristic function of $Y$ in terms of the derivative of the characteristic function of $X$:</p> \[\begin{align*} \forall t \in \R, &amp;&amp; \phi_Y(t) &amp;= \E(e^{itY}) \\ &amp;&amp;&amp;= \int_{\R} e^{itx} \frac{x}{\E(X)} \,df_X(x) \\ &amp;&amp;&amp;= \frac{1}{i\E(X)}\int_\R\frac{d}{dt}(e^{itx}) \,df_X(x) \\ &amp;&amp;&amp;= \frac{1}{i\E(X)}\frac{d}{dt}\E(e^{itX}) \\ &amp;&amp;&amp;= \frac{1}{i\E(X)}\phi_X'(t) \end{align*}\] <p>where we can swap the derivative and the expectation by the Leibniz rule since $X$ is non-negative and therefore $\abs{Xe^{itX}} \leq X$ which is integrable.</p> <h3 id="summary-table">Summary table</h3> <table> <thead> <tr> <th>Property</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>Density</td> <td>$f_Y(x) = \frac{x}{\E(X)}f_X(x)$</td> </tr> <tr> <td>$n$-th moment</td> <td>$\E(Y^n) = \frac{\E(X^{n+1})}{\E(X)}$</td> </tr> <tr> <td>Expectation</td> <td>$\E(Y) = \E(X) + \frac{\text{Var}(X)}{\E(X)}$</td> </tr> <tr> <td>Characteristic function</td> <td>$\phi_Y(t) = \frac{1}{i\E(X)}\phi_X’(t)$</td> </tr> </tbody> </table> <h2 id="vwt-of-specific-distributions">VWT of specific distributions</h2> <h3 id="exponential-and-gamma-distributions">Exponential and Gamma distributions</h3> <p>If $X$ has an exponential distribution with parameter $\lambda$, then:</p> \[\begin{align*} \forall x \in \R^{+}, &amp;&amp; f_Y(x) &amp;= \frac{x}{\E(X)}f_X(x) \\ &amp;&amp;&amp;= \frac{x}{1/\lambda}\lambda e^{-\lambda x} \\ &amp;&amp;&amp;= \lambda^2 x e^{-\lambda x} \end{align*}\] <p>This is the density of a Gamma distribution with shape parameter 2 and rate parameter $\lambda$.</p> <p>Since the exponential distribution is a special case of the Gamma distribution with shape parameter 1, this is a particular case of a more general result: the VWT of a Gamma distribution with shape parameter $\alpha$ and rate parameter $\lambda$ is a Gamma distribution with shape parameter $\alpha + 1$ and rate parameter $\lambda$.</p> <h3 id="poisson-distribution">Poisson distribution</h3> <p>If $X$ has a Poisson distribution with parameter $\lambda$, then:</p> \[\begin{align*} \forall k \in \N, &amp;&amp; f_Y(k) &amp;= \frac{k}{\E(X)}f_X(k) \\ &amp;&amp;&amp;= \frac{k}{\lambda}\frac{\lambda^k e^{-\lambda}}{k!} \\ &amp;&amp;&amp;= e^{-\lambda}\frac{\lambda^{k-1}}{(k-1)!} \\ &amp;&amp;&amp;= f_{X+1}(k) \end{align*}\] <p>Thus $Y$ has the same distribution as $X+1$ (a Poisson distribution with parameter $\lambda$ shifted by 1).</p> <p>This result has an interesting “memoryless” interpretation. If for example the number of children per family were poisson-distributed, then asking a random child how many siblings they have (not including themselves) would give you the same distribution as asking a random family how many children they have. This is caused by the fact that in a poisson process, conditioning on the fact that there is at least one increment (i.e. one child) in a time interval does not change the distribution of the number of other increments (i.e. siblings) due to its memoryless property.</p> <h3 id="uniform-and-beta-distributions">Uniform and Beta distributions</h3> <p>If $X$ has a uniform distribution on the interval $[0,1]$, then:</p> \[\begin{align*} \forall x \in [0,1], &amp;&amp; f_Y(x) &amp;= \frac{x}{\E(X)}f_X(x) \\ &amp;&amp;&amp;= 2x \end{align*}\] <p>Which is the same distribution as the maximum of two independent $\mathcal{U}([0,1])$ random variables.</p> <p>It is also the density of a Beta distribution with shape parameters 2 and 1, a special case of a more general result: if $X$ has a Beta distribution with shape parameters $\alpha$ and $\beta$, then:</p> \[\begin{align*} \forall x \in [0,1], &amp;&amp; f_Y(x) &amp;= \frac{x}{\E(X)}f_X(x) \\ &amp;&amp;&amp;= \frac{x}{\alpha/(\alpha + \beta)}\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha - 1}(1-x)^{\beta - 1} \\ &amp;&amp;&amp;= \frac{\Gamma(\alpha + \beta + 1)}{\Gamma(\alpha + 1)\Gamma(\beta)}x^{\alpha}(1-x)^{\beta - 1} \end{align*}\] <p>Which is the density of a Beta distribution with shape parameters $\alpha + 1$ and $\beta$.</p> <h3 id="summary-table-1">Summary table</h3> <table> <thead> <tr> <th>Distribution</th> <th>VWT</th> </tr> </thead> <tbody> <tr> <td>Gamma($\alpha$, $\lambda$)</td> <td>Gamma($\alpha + 1$, $\lambda$)</td> </tr> <tr> <td>$\rightarrow$ Exponential($\lambda$)</td> <td>Gamma(2, $\lambda$)</td> </tr> <tr> <td>Poisson($\lambda$)</td> <td>Poisson($\lambda$) + 1</td> </tr> <tr> <td>Beta($\alpha$, $\beta$)</td> <td>Beta($\alpha + 1$, $\beta$)</td> </tr> <tr> <td>$\rightarrow$ Uniform([0,1])</td> <td>Beta(2, 1)</td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="math"/><summary type="html"><![CDATA[Formalizing a statistical paradox as a probability transform and deriving its properties]]></summary></entry></feed>